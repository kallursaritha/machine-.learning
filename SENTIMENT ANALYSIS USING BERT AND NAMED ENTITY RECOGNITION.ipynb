{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d67274",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS USING BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea0b37",
   "metadata": {},
   "source": [
    "Sentiment analysis, also known as opinion mining, is a natural language processing (NLP) technique that involves determining the sentiment or emotional tone expressed in a piece of text. It aims to classify the subjective information present in text as positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e910401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60ec54c3f7845b4aa7cb870c0f80da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarithak\\AppData\\Local\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sarithak\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9647475b50f4dbaae64e2bdea67dccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30a12ab0572435ab40e0e8a5db7a76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c593ff2cfb542cf916bf3dcb3357294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 for binary sentiment classification\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Input text for sentiment analysis\n",
    "text = \"I really enjoyed the movie, it was great!\"\n",
    "\n",
    "# Preprocess text\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encoded_input['input_ids'].to(device)\n",
    "attention_mask = encoded_input['attention_mask'].to(device)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    predicted_labels = torch.argmax(logits, dim=1)\n",
    "    sentiment_label = \"Positive\" if predicted_labels.item() == 1 else \"Negative\"\n",
    "\n",
    "# Print sentiment label\n",
    "print(\"Sentiment:\", sentiment_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e0ec65",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe3283",
   "metadata": {},
   "source": [
    "Word2Vec is a popular algorithm for learning word embeddings, which are dense vector representations of words in a continuous vector space. It was introduced by Tomas Mikolov et al. in 2013 at Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bb2964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 1 1 1 1 1 1 1]]\n",
      "['best', 'for', 'guru99', 'is', 'love', 'online', 'site', 'the', 'to', 'tutorials', 'visit']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarithak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "data_corpus=[\"guru99 is the best site for online tutorials. I love to visit guru99.\"]\n",
    "vocabulary=vectorizer.fit(data_corpus)\n",
    "X= vectorizer.transform(data_corpus)\n",
    "print(X.toarray())\n",
    "print(vocabulary.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70fdc43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'chocolate':\n",
      "eat 0.1315944939851761\n",
      "cream 0.06800692528486252\n",
      "to 0.04157429561018944\n",
      "ice -0.01351268868893385\n",
      "like -0.013528075069189072\n",
      "love -0.04461246356368065\n",
      "I -0.11166319251060486\n",
      "\n",
      "Vector arithmetic: 'chocolate' + 'ice' - 'cream' =\n",
      "to 0.17760558426380157\n",
      "eat 0.1094869002699852\n",
      "love 0.07128658890724182\n",
      "like 0.05822988972067833\n",
      "I -0.09019022434949875\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Training data\n",
    "sentences = [[\"I\", \"love\", \"chocolate\"],\n",
    "             [\"I\", \"love\", \"ice\", \"cream\"],\n",
    "             [\"I\", \"like\", \"to\", \"eat\", \"chocolate\", \"ice\", \"cream\"]]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar(\"chocolate\")\n",
    "print(\"Similar words to 'chocolate':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)\n",
    "\n",
    "# Perform vector arithmetic\n",
    "result = model.wv.most_similar(positive=[\"chocolate\", \"ice\"], negative=[\"cream\"])\n",
    "print(\"\\nVector arithmetic: 'chocolate' + 'ice' - 'cream' =\")\n",
    "for word, similarity in result:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eeaaf3",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f521c3",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a natural language processing (NLP) technique used to identify and classify named entities in text. Named entities are real-world objects such as persons, organizations, locations, dates, quantities, and other specific categories. NER involves automatically detecting and classifying these entities within a given text or document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d6b6b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entities:\n",
      "Apple Inc. ORG\n",
      "New York City GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text for named entity recognition\n",
    "text = \"Apple Inc. is planning to open a new store in New York City.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "named_entities = []\n",
    "for ent in doc.ents:\n",
    "    named_entities.append((ent.text, ent.label_))\n",
    "\n",
    "# Print the named entities\n",
    "print(\"Named entities:\")\n",
    "for entity, label in named_entities:\n",
    "    print(entity, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c4791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
